\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\usepackage{caption}
\usepackage{color}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{charter}
\usepackage{environ}
\usepackage{tikz}
\usepackage[hidelinks]{hyperref}
\renewcommand\familydefault{\sfdefault}
\usetikzlibrary{calc,matrix}

\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\makeatletter% Set up caption and labels for lstlistings
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{greatblue}{\parbox{\textwidth}{\hspace{1cm}#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\let\matamp=&
\catcode`\&=13
\makeatletter
\def&{\iftikz@is@matrix
  \pgfmatrixnextcell
  \else
  \matamp
  \fi}
\makeatother

\newcounter{lines}
\def\endlr{\stepcounter{lines}\\}

\newcounter{vtml}
\setcounter{vtml}{0}

\newif\ifvtimelinetitle
\newif\ifvtimebottomline
\tikzset{description/.style={
  column 2/.append style={#1}
 },
 timeline color/.store in=\vtmlcolor,
 timeline color=red!80!black,
 timeline color st/.style={fill=\vtmlcolor,draw=\vtmlcolor},
 use timeline header/.is if=vtimelinetitle,
 use timeline header=false,
 add bottom line/.is if=vtimebottomline,
 add bottom line=false,
 timeline title/.store in=\vtimelinetitle,
 timeline title={},
 line offset/.store in=\lineoffset,
 line offset=4pt,
}

\NewEnviron{vtimeline}[1][]{%
\setcounter{lines}{1}%
\stepcounter{vtml}%
\begin{tikzpicture}[column 1/.style={anchor=east},
 column 2/.style={anchor=west},
 text depth=0pt,text height=1ex,
 row sep=1ex,
 column sep=1em,
 #1
]
\matrix(vtimeline\thevtml)[matrix of nodes]{\BODY};
\pgfmathtruncatemacro\endmtx{\thelines-1}
\path[timeline color st] 
($(vtimeline\thevtml-1-1.north east)!0.5!(vtimeline\thevtml-1-2.north west)$)--
($(vtimeline\thevtml-\endmtx-1.south east)!0.5!(vtimeline\thevtml-\endmtx-2.south west)$);
\foreach \x in {1,...,\endmtx}{
 \node[circle,timeline color st, inner sep=0.15pt, draw=white, thick] 
 (vtimeline\thevtml-c-\x) at 
 ($(vtimeline\thevtml-\x-1.east)!0.5!(vtimeline\thevtml-\x-2.west)$){};
 \draw[timeline color st](vtimeline\thevtml-c-\x.west)--++(-3pt,0);
 }
 \ifvtimelinetitle%
  \draw[timeline color st]([yshift=\lineoffset]vtimeline\thevtml.north west)--
  ([yshift=\lineoffset]vtimeline\thevtml.north east);
  \node[anchor=west,yshift=16pt,font=\large]
   at (vtimeline\thevtml-1-1.north west) 
   {\text{\vtimelinetitle}};
 \else%
  \relax%
 \fi%
 \ifvtimebottomline%
   \draw[timeline color st]([yshift=-\lineoffset]vtimeline\thevtml.south west)--
  ([yshift=-\lineoffset]vtimeline\thevtml.south east);
 \else%
   \relax%
 \fi%
\end{tikzpicture}
}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{greatblue}{RGB}{91,155,215} 
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstset{
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  backgroundcolor=\color{backcolour},
  upquote=true,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}

\title{%
  Google Summer of Code 2019
    \\ ~\\
    \large Block header parsing tool
    }

\author{Arpit Gupta}

\date{\today}

\begin{document}
\maketitle
\section{Introduction}
\label{sec:introduction}

Presently if a new block header file is created, we also need to create a YAML file in order to create a block which could be then viewed in the GNURadioCompanion (GRC). Also there is must be mechanism to check the validity of the header (.h) files specifically to GNU Radio by parsing them. Thus implied we need a tool to parse them.
\\
So, it would be great to create a tool which not only allows to create YAML files for the GRC but also could give us an abstract representation of the block by analysing the the whole header file code, which would be in the form of a tree, thus making the code readable in an hierarchical fashion. Once we get the abstract structure, it could be easily parsed for any further use.
\\
GRModtool is currently a powerful CLI tool and so this could be easily used for our purpose if the tool is built within it as another one of it's utility.
\\
The best tools that are presently being used to parse C++ in python are libclang with python and pygccxml, so these tools will be used to parse GNU Radio header files.

\subsection{Primary features of the project}
\begin{enumerate}
\item Implementation of both the parsing tools (libclang with python and pygccxml).
\item Abstract the implementation details of both parsers and creating abstract representation of the header files.
\item Extend GRModtool CLI to provide support for this tool. Make GRModtool to use this code itself instead of the built-in code.
\item Parse the abstract representation, create YAML files for GRC, make a script to change or add new optional YAML key parameters.
\item Extend it's utility to take a list of files as input, or even a whole directory and parse them and also extend it's utility to different file types, with this tool deciding how to manage each data-file type by its own (if time permits)
\end{enumerate}

\section{Proposed Workflow}
\label{sec:theory}
Initially, I will be doing the ground work by setting up the abstract implementation of both the tools with the use of GRModtool command line interface. Then I will be working on parsing the abstract representation and storing information in a python dictionary. Finally, I will be creating YAML files for the GRC.

\subsection{Implementation of parsing tools}
As GNU Radio API (headers) require us to use C++ for compiling, not straight C, so we need parsers in python to parse C++, so libclang in python and pygccxml both do an excellent job for this purpose and are especially good for GNU Radio header files because of their brilliant, elegant yet simple and most importantly their generic structure.

\subsubsection{Implementation of pygccxml}
Pygccxml provides a simple framework to navigate C++ declarations, using Python classes. It just needs to get access to the namespace in the file that we require to parse and we are good to go. It automatically work it's way down the hierarchy parsing all the variables, function calls and so on. It is compatible with Python 2.7, 3.4, 3.5, pypy and pypy3.
\\
Implementation of pygccxml is as shown below used for parsing a header file from a gr-directory:
\begin{lstlisting}
from pygccxml import utils
from pygccxml import declarations
from pygccxml import parser


def pygccxml_parser(filename, gr_directory):
    """
    Implementation of pygccxml parser
    """
    generator_path, generator_name = utils.find_xml_generator()

    # Configure the xml generator
    xml_generator_config = parser.xml_generator_configuration_t(
        xml_generator_path=generator_path,
        xml_generator=generator_name)
    filename = filename

    # Parse the c++ file
    decls = parser.parse([filename], xml_generator_config)

    # Get access to the 'analog' namespace in 'gr'
    global_namespace = declarations.get_global_namespace(decls)
    ns = global_namespace.namespace("gr")
    gr_block_type = gr_directory.split("-")[1]
    gr_namespace = global_namespace.namespace(gr_block_type)
\end{lstlisting}
\subsubsection{Implementation of Libclang with python}
Libclang is a tool used for analyzing C/C++/ObjC code at the compiler level which means it can also be used to analyze all the standard C++ libraries too. Libclang with python bindings provide us pythonic way of implementing classes and functions to parse C++ files. This tool is also compatible with Python2.7, 3.4, 3.5, pypy and pypy3.
\\
Implementation of libclang in python is as shown below for used for parsing a header file from a gr-directory:
\begin{lstlisting}
import sys
from clang.cindex import Index
from pprint import pprint


def get_cursor_id(cursor, cursor_list = []):
    """
    Cursor point to the current node
    """
    if cursor is None:
        return None

    for i,c in enumerate(cursor_list):
        if cursor == c:
            return i
    cursor_list.append(cursor)
    return len(cursor_list) - 1

def get_info(node, depth=0):
    """
    Recursively get info of all the nodes
    """
    children = [get_info(c, depth+1)
                for c in node.get_children()]
    return { 'id' : get_cursor_id(node),
             'kind' : node.kind,
             'usr' : node.get_usr(),
             'spelling' : node.spelling,
             'location' : node.location,
             'extent.start' : node.extent.start,
             'extent.end' : node.extent.end,
             'is_definition' : node.is_definition(),
             'definition id' : get_cursor_id(node.get_definition()),
             'children' : children }


index = Index.create()
tu = index.parse(sys.argv[1], args=['-x', 'c++'])
pprint(('nodes', get_info(tu.cursor)))
\end{lstlisting}
Libclang parses all the standard C++ header files thus generating a very long AST which thus makes them very difficult to parse, besides it provides us with redundant information of standard libraries. So, The way I figured out is to create a list of standard C++ header files and thus exclude them from being parsed in the header file itself.

\subsection{Abstract implementation of parsers}
It is due to the weird syntax of C++, all the current C++ parsers are not perfect, thus need for their abstract implementation. Abstracting the details of implementation of both the parsers is necessary so that we can replace them with some other tool or might even easily extend it, if need be.
\\
Abstract implementation of both the parsers from CLI is as shown below:
\begin{lstlisting}
def AbstractASTGenerator(parser):
    """
    Abstract parser implementation
    """
    try:
        if parser.lower() == 'clang':
            ClangASTGenerator()
        else if parser.lower() == 'pygccxml':
            PygccxmlASTGenerator()
        # Can be easily extended futher if required
    except Exception as e:
        pass

def InitailParserInput():
    """
    Parser Type Input
    """
    parser_list = [
        'clang',
        'pygccxml',
    ]
    parser = input('Parser type(' + '/'.join(parser_list) + '): ')
    if parser.lower() in parser_list:
        ast = input('Do you require an AST file? (y/n): ')
        if ast.lower() == 'y':
            AbstractASTGenerator(parser.lower())
        # Similarly other functions can be implemented
    else:
        raise Exception('Please specify a valid parser name!')
\end{lstlisting}
 As shown above, other functionalities can be abstracted in such a way such that any other parser after being implemented can be easily called to use it's functionality in form of such as python functions to get their make function, function calls, i/o signatures.
 \\
 After the abstract implementation we get Abstract Syntax Tree (AST) for the header code to work with. AST provides us with the whole complete tree structure of the header file to be parsed.
 \\
 Example AST generated by both the parsers for a GNU Radio header file (agc2\_cc\_impl.h) are:
 \\
 \begin{itemize}
\item \href{https://github.com/aru31/GSoC-Proposal/blob/master/clang-ast.txt}{Clang-generated-AST}
\item \href{https://github.com/aru31/GSoC-Proposal/blob/master/pygccxml-ast.txt}{Pygccxml-generated-AST}
 \end{itemize}
\subsection{Extend GRModtool CLI to provide support for this tool}
GRModtool in a tool in GNU Radio that provides a CLI for creating OOT modules. It provides various features, for example creating new OOT modules, adding various blocks, YAML generator and much more. 
\\
So, block header parsing tool being a command line utility can easily be integrated with GRModtool, also extend it's code base for the new tool to be in use.
\\
Another script can be called from CLI, for example from `gr\_modtool parse` to serve our purpose.
\\
I have already gone through the code of GRModtool, which is brilliantly written thus making it really to extend it.
\begin{itemize}
\item CLI will be implemented in the cli module of GRModtool, extending the use of click and click plugins.
\item Implemetation of the backend code of both the parsers will be implemented in the core modules of GRModtool.
\end{itemize}
GRModtool already consists a parser written in parser\_cc\_block.py file to parse C++ files. It is although written elegantly but the code is limited only to parse GNU Radio cc files and if thought to be extended further as a generic API, it cannot be done because of the code structure which is written by analysing the code pattern limited to GNU Radio headers, whereas compiler level parsing is required in order to achieve the above goal which is achieved by both the parsers.
\\ \\
Manual syntax check as shown below (code snippet from parser\_cc\_block.py) is no longer required as earlier done while parsing C++ files, because both parsers throw compiler exceptions in case of syntax error (if any).
\begin{lstlisting}
while not end_of_list:
    # Keep track of (), stop when reaching final closing parens
    if not in_string:
        if c[i] == ')':
            if parens_count == 0:
                if read_state == 'type' and len(this_type):
                    raise ValueError(
                        'Found closing parentheses before finishing '
                        'last argument (this is how far I got: {})'.format \
                        (str(param_list))
                    )
\end{lstlisting}
This code is no longer required to be implemented for parenthesis check of every function defined.
\subsection{Parse the abstract representation, Create YAML files for GRC}
This subsection consists of the most important part of the whole project that is to correctly parse the header files by analysing all the function signatures, raising proper exceptions wherever required according to the convention followed in GNU Radio. Because of the use of two different parsers in this project, they generate AST in different format, thus different parsing strategies need to be implemented for both of them.
\\
\subsubsection{Parsing pygccxml generated AST}
Pygccxml generates AST (txt file mentioned in section 2.2) in form of tree type data structure because of the implementation of helper-print function in it to print all the declarations.
The best way to parse the AST generated by pygccxml is to parse through the top level hierarchy of the header file and then reach the bottom level that is parsing different defined functions and function declarations.
\\ \\
Architecture of pygccxml generated AST parser is as shown below:
\begin{itemize}
\item Parse through all the namespaces that are defined in the header file.
\item Parse through the classes defined in the namespace used.
\item  Parse through the getters/setters defined in a class (public/protected/private).
\item  Parse through the make function defined in the public constructor of the class.
\item Finally parse through different variables and functions defined in constructors (defined as parameters in YAML files).
\end{itemize}
Starting through the top level, example parser implementation for the pygccxml generated AST is as shown below, not complete parser though, but minor part of it, complete parser will be implemented as part of my project:
\begin{lstlisting}
def namespace_parsing(filename, gr_directory):
    """
    Function to parse the namespace defined in the file
    """
    with open(filename, 'r') as file_object:
        lines = file_object.readlines()
    namespace_name = []
    namespace_index = []
    gr_block_type = gr_directory.split("-")[1]
    for line in lines:
        if 'namespace_t' in line:
            namespace_name.append(line.split(": ")[1].strip().replace("'", ""))
            namespace_index.append(lines.index(line))
    if namespace_name[0] == gr_block_type:
        lines = lines[0: namespace_index[1]]
    else if namespace_name[1] == gr_block_type:
        lines = lines[namespace_index[1]: len(lines)]
        else:
        raise Exception('Wrongly specified namespace, must be {}'.format(gr_block_directory))
\end{lstlisting}
Similarly, rest of the parsing API will be defined according to the given architecture during my GSoC coding period.
\\
\subsubsection{Parsing libclang generated AST}
Libclang in python generates ast (txt file mentioned in section 2.2) as a JSON which is really easy to parse.
Architecture of libclang generated AST parser is as shown below:
\begin{itemize}
\item To parse the methods defined, we need to get all the child nodes with key-value pairs defined as `'kind': CursorKind.CXX\_METHOD` from the JSON.
\item To parse , we need to get all the child nodes with key-value pairs defined as `'kind': CursorKind.PARM\_DECL`, from the JSON.
\end{itemize}
Thus we get all the possible variable, function declarations in the header file but we need their data types, return types too.
\begin{lstlisting}
def traverse(node):
    """
    function that helps cursor traverse through all the nodes
    """
    for child in node.get_children():
        traverse(child)
    print('Found {} [line={}, col={}]'.format(node.displayname, node.location.line, node.location.column))
\end{lstlisting}
The output of this function will provide us with data types of all the variables and return types of different function declarations defined in the block header, which could be sorted wth the use of parser architecture.
\\
\subsubsection{Raising proper exceptions while parsing}
Raising proper exceptions is also necessary, because the header file should be defined according to GNU Radio header API standard and then should be allowed to create YAML from them. Few of these are:
\\
\begin{itemize}
\item A public destructor must be defined along-with a public constructor for main block header files so that it is called automatically when the object goes out of scope.
\item  A `work()` function must be declared with return type `int` with arguments `gr\_vector\_const\_void\_star \&input\_items`, `gr\_vector\_void\_star \&output\_items` and  `noutput\_items`.
\end{itemize}
\subsubsection{Creating YAML files for GRC}
Once the data is parsed, store it in a python dict, create YAML files as per the layout of standard YAML files in GNU Radio. 
\\
YAML file architecture in GNU Radio consists of these following keys:
\\
\begin{itemize}
\item id, label, flags, parameters, inputs, outputs, templates, file\_format.
\item  cpp\_templates (only if flag includes cpp), documentation (optional).
\end{itemize}
Values of the keys namely id, label, flags (will be a CLI input) and file\_format are implemented as shown below:
\begin{lstlisting}
def file_name_parsing(file_name):
    """
    Function for generating YAML architecture
    """
    label_dict = {
        'c': '(Complex)',
    }
    label_part = (file_name.split("_"))
    if len(label_part[-1]) < 2:
        if label_part[-1] not in label_dict:
            label_part.pop()
        else:
            label_part[-1] = label_dict[label_part[-1]]
    data = {}
    data['id'] = file_name.split(".")[0].lower()
    data['label'] = (' '.join(label_part)).title()
    data['flags'] = ['python'] # optional cpp templates
    data['file_format'] = 1
    return data
\end{lstlisting}
All the other key-values for YAML are the output of parsed data.
\\ \\
A script for adding optional key-value parameters must also be implemented (in case required), for example if cpp flag is removed, cpp\_templates key-value should be removed from the YAML file.
\subsection{Manage different data-file type (if time permits)}
One of the features that would be really good to implement is to take a list of files as an input, put them in a queue and parse them, reducing the effort to parse headers one at a time.
\\
Also, utility of the tool could be extended to implement a parser for .cc files in GNU Radio, just different text-parsing functions need to be implemented with almost the same code, not a difficult job at all.
\section{Timeline}
I will utilize the period of community bonding to familiarize myself with the GNU Radio community. I will also make sure to gain a deeper insight of the source code. This will enable me to contribute more efficiently to the community. Moreover, I will figure out ways the header parsing tool in the most possible efficient way by using efficient python libraries for my purpose, squashing all (if not most) the bugs before it's completion. Additionally, I will define minute details of the project so that I face minimal difficulty in the coding period.
\\ \\
The necessary documentation will be done in parallel with the development. There is 13-week long coding period. I have my holidays in the months of June, July and August so I'll work full time during this period, i.e., around 35-40 hours a week. I have made my deliverables accordingly on weekly basis. 
\\ \\
The expected timeline for my project is given below:
\\ \\

\begin{vtimeline}[description={text width=10cm}, 
 row sep=6ex, 
 use timeline header,
 timeline title={Timeline of the project}]
May 6 - May 27 & Define minute details of the project and build a sample parser architecture.\endlr
May 27 - June 3 & Initialize the implementation of both the parsers to generate AST.\endlr
June 3 - June 10 & Extend GRModtool to make it compatible with the new tool.\endlr
June 10 - June 17 & Abstract the implementation of both the parsers, defining all the helper functions required.\endlr
June 17 - June 24 & Complete the first three primary features of this project, and make GRModtool CLI ready to take proper inputs (of course still without actually text parsing).\endlr
June 24 - July 1 & Start working on parsing pyggcxml generated AST. \endlr
July 1 - July 8 & Start working on parsing libclang generated AST and complete the implementation of the previous one. \endlr
July 8 - July 15 & Complete the implementation on parsing AST generated by both parsers with thorough testing. \endlr
July 15 - July 22 & Creating YAML files for the GRC with properly raised exceptions. \endlr
July 22 - July 29 & Make a script to edit the optional YAML keys. \endlr
July 16 - Aug 5 & Thoroughly test the block header parsing tool, buffer time for completing the remaining tasks.\endlr
Aug 5 - Aug 12 & Start working on the extended part that is to take a file list as an input and parse them. \endlr
Aug 12 - Aug 19 & Start working on extending this tool for .cc files. \endlr
Aug 19 - Aug 26 & Complete the project and submit the final report. \endlr
\end{vtimeline}
\pagebreak

\section{Deliverables of GSoC 2019}
The deliverables of the GSoC project are as follows:
\begin{itemize}
\item Implement abstract parsers compatible with python 3.
\item Extend Command Line Interface of GRModtool to support the new tool that would abstract the use of parsers.
\item Properly parsing the generated AST and creating YAML files from command line itself for GRC.
\item Implement the complete tool with thorough testing.
\end{itemize}

\subsection{Milestones}
\begin{itemize}
\item Phase-1: Making GRModtool capable of doing all the new tasks, abstract the implementation of the parsers.
\item Phase-2: Successfully parsing AST generated by the parsers.
\item Final Evaluation: Completion of the tool, successfully creating YAML files for the GRC, making the tool functional as a generic API so that it would be easy to work on this tool further, and could be used by other tools.
\end{itemize}

\subsection{Review/Merge Cycle}
The code can be reviewed as per the proposed timeline whereas it can be merged according to timeline stated below:
\begin{itemize}
\item June 24: Merge code for extended GRModtool utility and abstract parser structure.
\item July 22: Merge code for parsing AST generated by parsers.
\item August 19: Merge the entire block header parsing tool along with it's extended utilities (if possible).
\end{itemize}

\subsection{Automated Testing}
The code will be thoroughly tested locally at almost every step of the whole architecture, test scripts will also be written to ensure all the third-party packages are correctly installed and will work on building a minimal Python QA that creates a module, adds a Python source and Python QA file to it, runs the QA test with randomly generated numbers in the Python source and then deletes the entire directory. The code will also be compatible with standard pylint rules.

\section{Acknowledgement}
I have thoroughly gone through the GSoC StudentInfo page and GSoC Manifest page. I hereby assure that I will abide by the rules and regulations. I also accept the three strikes rule and the details mentioned.
\\
I also assure that I will communicate with the assigned mentor regularly, maintain thorough transparency and keep my work up to date.

\section{License}
The entire code during the coding period will be transparent, i.e., available on Github. The code submitted will be GPLv3 licensed.

\section{Personal Details and Experience}
I am a second year undergraduate at Indian Institute of Technology Roorkee. My areas of interest are software development, signal processing and deep learning. I am proficient in Python, C++, JAVA, JavaScript, and PHP. I am familiar with git environment as I work regularly on Github. I haven't contributed much to open source but since we all know \textbf{Cyberspectrum is the best spectrum}, Ill really like to contribute to GNU Radio and make it as my first remarkable experience. I will not get any extra credits for the GSoC project. I am proficient in two human languages including English.
\\
I have the experience of working closely with a team as I am an active member of \href{http://img.channeli.in/}{\textbf{Information Management Group}} at IIT Roorkee, a bunch of passionate enthusiasts who manage the \href{https://www.iitr.ac.in/}{\textbf{institute main website}}, internet and intranet activities of the university and the placement portal. My major project as a part of the group is Lectures and Tutorials Portal (Lectut), an intranet-based study portal,used by unique 4k campus students and faculty every month with an aim of assisting them to achieve their academic goals. The project has been implemented on django, django-rest (python based framework).
\\ \\
I am also a member of \textbf{Vision and Language Group} which is aimed at spreading the culture of algorithms and competitive programming among people both in and outside IIT Roorkee by organizing contests, delivering lectures, etc.
\\ \\
I started off with GNU Radio in January 2019. To get familiarized with the code, I made the following contributions to the code base:
\begin{enumerate}
\item Pull request \href{https://github.com/gnuradio/gnuradio/pull/2339}{\#2339}: gr\_modtool: remove unused code in rename.py
\item Pull request \href{https://github.com/gnuradio/gnuradio/pull/2338}{\#2338}: Update README.md to provide instructions for building GR with support of python3.x
\item Pull request \href{https://github.com/gnuradio/gnuradio/pull/2350}{\#2350}: grc: fix for GRC's block hotkeys
\item Pull request \href{https://github.com/gnuradio/gnuradio/pull/2352}{\#2352}: grc: parse prefix for numeric values
\item Pull request \href{https://github.com/gnuradio/gnuradio/pull/2373}{\#2373}: grc: Fix save as for existing grc file
\item Pull request \href{https://github.com/gnuradio/gnuradio/pull/2330}{\#2330}: Add C++ generation support to gr-digital
\item Pull request \href{https://github.com/gnuradio/gnuradio/pull/2343}{\#2343}: Add C++ generation support to gr-filter
\item Pull request \href{https://github.com/gnuradio/gnuradio/pull/2361}{\#2361}: gr\_modtool: Fix for parameter wrap in cli inputs
\item Pull request \href{https://github.com/gnuradio/gnuradio/pull/2398}{\#2398}: grc: Fix color for input boxes in parameter widget according to dark-gtk themes
\end{enumerate}
I will always be available on email or Google Hangouts for any kind of discussion or query.
\\
I am highly interested to contribute to GNU Radio after the GSoC period. After the period, I'll mainly focus on extending it's utility to different types of files, with the tool deciding how to manage each datafile type by its own. I'll always be available for fixing the bugs that come up in the block header file tool. 
\\ \\
Here is the \href{https://aru31.github.io/resume.pdf}{link} to my CV.
\\
\subsection{Other Details}
\begin{tabular}{ l l l }
 Address & : & Roorkee, Uttarakhand, India \\
 Email & : & \href{mailto:guptarpit1997@gmail.com}{guptarpit1997@gmail.com}  \\  
 Github & : & \href{https://github.com/aru31/}{https://github.com/aru31/} \\
 LinkedIn & : & \href{https://www.linkedin.com/in/aru31/}{https://www.linkedin.com/in/aru31/} \\
 Codechef & : & \href{https://www.codechef.com/users/aru31121997}{ https://www.codechef.com/users/aru31121997} \\
\end{tabular}

\section{Conclusion}
Block header parsing tool is one of the tools necessarily required because this would not only help in developing bug free header files, but also according to the conventions followed in GNU Radio that too from a CLI without actually using GRC. This tool itself could also be used as an API by any other tool (external to GNU Radio) to parse any header file. Also, no more manually creating YAML files for GRC.

\end{document}